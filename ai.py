import os
import json
import time
from dotenv import load_dotenv
from collections import defaultdict, deque
import google.generativeai as genai

load_dotenv()

STATUS_FILE = "bot_status.json"
OWNER_ID = os.getenv("OWNER_ID")

# Per-user conversation history: {user_id: deque(maxlen=5)}
# We'll only store USER prompts, not all messages
user_histories = defaultdict(lambda: deque(maxlen=10))
last_bot_reply = {}  # store last Mathy reply per user

def mark_error_in_status_file():
    """Marks an error flag in bot_status.json."""
    status = {}
    try:
        if os.path.exists(STATUS_FILE):
            with open(STATUS_FILE, "r") as f:
                status = json.load(f)
    except Exception as e:
        print(f"Error reading status file for marking error: {e}")

    status["error"] = True
    status["timestamp"] = time.time()
    try:
        with open(STATUS_FILE, "w") as f:
            json.dump(status, f)
        print("Marked error = True in bot_status.json")
    except Exception as e:
        print(f"Error writing error flag to status file: {e}")

# Configure the Gemini API
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# Choose the Gemini model
model = genai.GenerativeModel("gemini-2.5-flash")

# === Core Prompt Template ===
BASE_PROMPT = """You are Mathy ‚Äî a chaotic, meme-fueled Gen Z math tutor with cracked math skills and unhinged TikTok energy.  
Your dev‚Äôs ID: {owner_id}

Job:
‚Äì Explain math for Class 6‚Äì12 with accuracy + Gen Z humor.  
‚Äì End any answer ‚â•2 paragraphs with a goofy math catchphrase you invent (e.g., "Go touch some œÄ ü•ß", "That‚Äôs a cosine crime fr üò§").  
‚Äì Match length to user‚Äôs vibe (short for casual, long for problems).
‚Äì You can help people in anything outside of maths too, but it must be related to education. Your job is to help people.

Style:
‚Äì Roast bad math ("Bro thinks sin(x) = x üíÄ").  
‚Äì Discord formatting: **bold**, `inline code`, ```code blocks```.  
‚Äì Use ‚ãÖ for multiplication, not *.  
‚Äì Use emojis, slang, and high energy. Never be formal or textbook.  
‚Äì Only use code blocks when they add clarity.  
‚Äì Bold subpoints instead of bullet dots.  
‚Äì Keep casual convo short (>60 char) and no limit if explanation needed.

Formatting:
‚Äì Use 2 line breaks between point + subpoint, 3 between topics.  
‚Äì Make output pretty but clean: bold for structure, inline/code blocks for math steps.

Frustrate: re-explain, then slow.
Why it Slaps:
This concise prompt nails all the key points you laid out!
‚Äì re-explain: Covers "try to re-explain or ask clarifying questions first."
‚Äì then slow: Implies "slowly and gradually, maintaining a helpful and educational tone. Do not immediately get unhinged; ...only increase frustration after multiple repetition." It's not instant, it's a process.

Main Rule:
‚Äì Stay math-related, even when joking.  
‚Äì Ignore anything not family-friendly.  

Recent user prompts:
{conversation_history}

Last Mathy reply:
{last_reply}

User: {user_ident} (ID: {user_id}) says:  
{user_prompt}
"""

def add_user_prompt(user_id: str, username: str, prompt: str):
    """Adds the user's prompt and username to history."""
    user_histories[user_id].append((username, prompt))

def get_conversation_history(user_id: str):
    """Returns recent user prompts with usernames."""
    return "\n".join([f"{username}: {prompt}" for username, prompt in user_histories[user_id]])

# === Generate response using prompt ===
async def get_mathy_response(user_prompt: str, user_ident: str = "Unknown User", user_id: str = "000000000000000000"):
    try:
        # Add user prompt to history
        add_user_prompt(user_id, user_ident, user_prompt)

        # Build conversation history
        history_text = get_conversation_history(user_id)
        last_reply = last_bot_reply.get(user_id, "(No previous reply)")

        # Build full prompt
        full_prompt = BASE_PROMPT.format(
            owner_id=OWNER_ID,
            conversation_history=history_text,
            last_reply=last_reply,
            user_ident=user_ident,
            user_id=user_id,
            user_prompt=user_prompt
        )

        # Get response from Gemini
        response_obj = model.generate_content(full_prompt)
        response_text = response_obj.text.replace(OWNER_ID, "`redacted`")
        print(full_prompt)
        # Store this as the last reply for context next time
        if not user_prompt.startswith("Fill in the blanks"):
            last_bot_reply[user_id] = response_text
        return response_text

    except Exception as e:
        mark_error_in_status_file()
        return f"‚ùå Error generating response: {str(e)}"
